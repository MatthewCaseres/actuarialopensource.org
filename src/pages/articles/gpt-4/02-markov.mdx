import { ArticleLayout } from '@/components/ArticleLayout'
import Image from 'next/future/image'

export const meta = {
  author: 'Matthew Caseres',
  date: '2023-04-25',
  title: 'Hosting chainladder-python on Streamlit',
  description:
    'We talk about deploying a Streamlit app with triangles from Chainladder',
}

export default (props) => <ArticleLayout meta={meta} {...props} />

# Markov Chains

<h1>Markov Chains</h1>

## What is a Markov chain?

A Markov chain is a stochastic process that models a sequence of events (or states) in which the probability of transitioning from one state to another depends only on the current state, and not on the past states. This property is called the Markov property or memorylessness. Markov chains can be either discrete or continuous, depending on whether the state space and time are discrete or continuous.

For discrete Markov chains, both the state space and time are discrete. The transition probabilities between states can be defined by a transition matrix, where the element P_ij represents the probability of transitioning from state i to state j.

Discrete Markov chains can be further classified as homogeneous or non-homogeneous:

Homogeneous Markov Chains: In a homogeneous Markov chain, the transition probabilities between states remain constant over time. The transition matrix, denoted by P, is time-invariant. The probability of transitioning from state i to state j after n time steps is given by the (i, j)-th entry of the matrix P^n.

Non-homogeneous Markov Chains: In a non-homogeneous Markov chain, the transition probabilities between states can change over time. The transition matrix may vary at each time step, denoted by P(t), where t represents the time step. Non-homogeneous Markov chains are more complex to analyze compared to homogeneous Markov chains, as the transition probabilities are time-dependent.

In summary, a Markov chain is a stochastic process modeling a sequence of events with the Markov property. Discrete Markov chains have discrete state space and time, and can be homogeneous (time-invariant transition probabilities) or non-homogeneous (time-varying transition probabilities).

## Random walks on discrete Markov chains

In the context of discrete Markov chains, a random walk is a specific type of stochastic process where an object or particle moves in a sequence of steps, with each step determined by a random process. The random walk can be thought of as a Markov chain, where the state space represents the possible positions of the object, and the transition probabilities define the likelihood of moving from one position to another.

There are several types of random walks, depending on the structure of the state space and the transition probabilities:

Simple Random Walk: In a simple random walk, the state space is typically a one-dimensional lattice (such as the integers). At each step, the particle moves to a neighboring position with equal probability. For example, on the integer lattice, the particle moves one step to the left or right with probability 1/2 each.

Random Walk on a Graph: In this case, the state space is represented by the vertices of a graph, and the transition probabilities are determined by the edges connecting the vertices. At each step, the particle moves from its current vertex to a neighboring vertex, chosen with a certain probability. The transition probabilities can be uniform (each neighbor is equally likely) or non-uniform (some neighbors are more likely than others).

Higher-dimensional Random Walks: Random walks can also occur in higher-dimensional spaces, such as two-dimensional grids or three-dimensional lattices. In these cases, the transition probabilities are defined by the possible moves between neighboring positions in the higher-dimensional space.

Continuous Random Walks: Although the original question focuses on discrete Markov chains, it is worth mentioning that random walks can also be continuous, such as the Wiener process (also known as Brownian motion) in continuous time and continuous state space.

Random walks have various applications in physics, computer science, finance, and other fields. They are often used to model phenomena such as diffusion, stock prices, and the movement of particles in a fluid. In the context of Markov chains, random walks exemplify the Markov property, as the next position of the particle depends only on its current position and not on its past trajectory.

## Classification of states and classes of states (absorbing, accessible, transition, irreducible, and recurrent)

In the context of discrete Markov chains, states can be classified based on their properties and their relationships to other states. This classification helps us understand the long-term behavior of the Markov chain. Here are the definitions of different classes of states:

Absorbing State: A state i is absorbing if once the process enters state i, it remains there forever. In other words, the transition probability of staying in state i is 1 (P_ii = 1) and the transition probabilities to other states are 0.

Accessible State: A state j is accessible from state i if there is a positive probability of reaching state j from state i in a finite number of steps. Mathematically, state j is accessible from state i if there exists a positive integer n such that the (i, j)-th entry of the matrix P^n is greater than 0.

Transition State: If state j is accessible from state i and state i is also accessible from state j, then states i and j are said to communicate with each other, and we can call them transition states.

Irreducible State: A Markov chain is irreducible if every state is accessible from every other state, meaning all states communicate with each other. In an irreducible Markov chain, the long-term behavior of the chain does not depend on the initial state.

Recurrent State: A state i is recurrent if, starting from state i, the expected number of visits to state i is infinite. In other words, once the process leaves state i, it is guaranteed to return to state i infinitely often. A state that is not recurrent is called transient. A transient state will be visited only a finite number of times before the process moves to a recurrent state.

These classifications help us analyze the long-term behavior of Markov chains and their convergence properties. For example, if a Markov chain is irreducible and all states are recurrent, then the chain has a unique stationary distribution, which describes the long-term behavior of the system regardless of its initial state. If a Markov chain has absorbing states, we can study the absorption probabilities and expected time to absorption, which are important in various applications such as reliability analysis and queuing theory.

## Transition step probabilities and stationary probabilities

In discrete Markov chains, transition step probabilities and stationary probabilities are two essential concepts that describe the chain's behavior over time.

Transition Step Probabilities: These probabilities describe the likelihood of transitioning from one state to another after a certain number of steps. For a homogeneous Markov chain, the transition step probabilities are given by the elements of the transition matrix P raised to the power of the number of steps, denoted as P^n, where n is the number of steps. The (i, j)-th entry of P^n represents the probability of transitioning from state i to state j after n steps. For non-homogeneous Markov chains, transition step probabilities are time-dependent and involve the product of transition matrices up to the desired time step.

Stationary Probabilities: A stationary (or invariant) probability distribution for a Markov chain is a probability distribution that remains unchanged as the chain transitions from one state to another. In other words, if π is a stationary probability distribution and P is the transition matrix, then:

πP = π

The stationary distribution provides insight into the long-term behavior of the Markov chain. For an irreducible and aperiodic Markov chain, there exists a unique stationary distribution, and the chain converges to this distribution as the number of steps approaches infinity, regardless of the initial state:

lim(n→∞) P^n = π

Stationary probabilities are useful in various applications, such as steady-state analysis of queuing systems, network traffic modeling, and population dynamics. The existence and uniqueness of a stationary distribution depend on the properties of the Markov chain, such as irreducibility, periodicity, and recurrence.

In summary, transition step probabilities describe the likelihood of transitioning between states after a certain number of steps and are given by the elements of the transition matrix raised to the power of the number of steps. Stationary probabilities represent a probability distribution that remains unchanged as the Markov chain evolves and provide insight into the chain's long-term behavior. The existence and uniqueness of stationary probabilities depend on the properties of the Markov chain.

## Gambler's ruin problem

The gambler's ruin problem is a classic problem in probability theory that models a gambler's situation in a simple game of chance. It is often used as an example to illustrate the concepts of random walks and discrete Markov chains.

In the gambler's ruin problem, a gambler starts with an initial fortune of $i, and plays a game with another player (such as a casino). For each round of the game, the gambler has a probability p of winning $1 and a probability q = 1-p of losing $1. The game continues until the gambler either loses all their money (fortune becomes 0) or reaches a target fortune of $N, at which point the gambler stops playing. The gambler's ruin problem aims to determine the probability that the gambler will reach the target fortune before going broke and the expected number of rounds before either outcome occurs.

This problem can be modeled as a random walk on a one-dimensional lattice, where each position represents the gambler's current fortune. The state space consists of the integers between 0 and N, inclusive. The gambler's fortune changes by +1 or -1 at each step, with probabilities p and q, respectively. The states 0 and N are absorbing states, as once the gambler reaches either of these states, the game ends.

To analyze the gambler's ruin problem, we can set up a system of equations based on the transition probabilities and solve for the probability of reaching the target fortune $N and the expected number of rounds. The probability of ruin (losing all the money) can be determined as:

P_ruin(i) = 1 - P_reach_N(i)

where P_reach_N(i) is the probability of reaching the target fortune $N starting from initial fortune $i.

For a fair game (p = q = 0.5), the probability of reaching the target fortune is:

P_reach_N(i) = i/N

For an unfair game (p ≠ q), the probability of reaching the target fortune is:

P_reach_N(i) = ( (q/p)^i - (q/p)^N ) / (1 - (q/p)^N)

The gambler's ruin problem demonstrates the principles of random walks and Markov chains, as well as the concept of absorbing states. It has applications in areas such as finance, economics, and gambling, and serves as a foundation for more complex models of risk and ruin.

## Branching processes

Branching processes are a class of stochastic processes used to model the growth and evolution of populations, particularly in contexts where reproduction and extinction events occur randomly. In the context of discrete Markov chains, branching processes can be thought of as Markov chains with discrete time steps and a potentially infinite state space.

The simplest form of a branching process is the Galton-Watson process, which is defined as follows:

There is an initial population of size Z_0.
Each individual in the population independently produces a random number of offspring according to a probability distribution, which is common to all individuals.
The offspring then form the next generation, and each of them independently produces offspring according to the same probability distribution as their parent.
The process continues in discrete time steps (generations).
In the Galton-Watson process, the state space consists of non-negative integers representing the population size at each generation. The transition probabilities are determined by the offspring distribution. The process is a Markov chain because the number of individuals in the next generation depends only on the current generation's size and not on the previous generations.

One of the main questions of interest in branching processes is the probability of extinction, i.e., the probability that the population eventually dies out. This can be determined by analyzing the offspring distribution's properties, such as its mean and variance.

For example, consider a Galton-Watson process with offspring distribution p_k, where p_k is the probability that an individual has k offspring. The mean of the offspring distribution is given by:

μ = Σ k \* p_k

The extinction probability can be calculated as the smallest non-negative solution to the following equation:

q = Σ p_k \* q^k

where q is the probability of extinction.

In summary, branching processes, particularly the Galton-Watson process, are a class of discrete Markov chains used to model the growth and evolution of populations with random reproduction and extinction events. Key properties of interest include the probability of extinction and the expected population size at each generation. These processes have applications in various fields, such as biology, demography, and epidemiology.

## Homogenous transition probabilities

Homogeneous transition probabilities refer to the transition probabilities of a homogeneous Markov chain. In a discrete homogeneous Markov chain, the state space and time are discrete, and the transition probabilities between states remain constant over time. This time-invariant property is what makes the transition probabilities homogeneous.

The transition probabilities are represented by a transition matrix, P, where the element P_ij represents the probability of transitioning from state i to state j. Since the transition probabilities are homogeneous, the matrix P remains constant for all time steps.

Homogeneous transition probabilities imply that the likelihood of transitioning between states does not depend on the time at which the transition occurs. This property simplifies the analysis of Markov chains, as the transition probabilities can be easily manipulated using matrix operations.

For example, the probability of transitioning from state i to state j after n time steps is given by the (i, j)-th entry of the matrix P^n, where P^n is the matrix P raised to the power of n.

In summary, homogeneous transition probabilities are the constant, time-invariant probabilities of transitioning between states in a discrete homogeneous Markov chain. These probabilities are represented by a transition matrix, P, which remains constant over time. Homogeneous transition probabilities simplify the analysis of Markov chains and are widely used in various applications, such as modeling stochastic processes, queuing theory, and network traffic modeling.

## Limiting probabilites

Limiting probabilities, also known as limiting distributions, refer to the probabilities of being in a certain state in a Markov chain as time goes to infinity. Limiting probabilities describe the long-term behavior of a Markov chain, provided that certain conditions are met. The limiting probabilities are closely related to stationary probabilities, which are probability distributions that remain unchanged as the chain transitions from one state to another.

For a discrete Markov chain with transition matrix P, the limiting probabilities can be represented by a row vector π, such that:

lim(n→∞) P^n = π

where P^n is the matrix P raised to the power of n. The limiting probabilities must satisfy the following equation:

πP = π

It is important to note that not all Markov chains have limiting probabilities. The existence and uniqueness of limiting probabilities depend on certain properties of the Markov chain, such as:

Irreducibility: The Markov chain is irreducible if every state is accessible from every other state, meaning there is a positive probability of transitioning between any pair of states in a finite number of steps.

Aperiodicity: The Markov chain is aperiodic if the greatest common divisor of the lengths of all possible cycles in the chain is 1. In other words, the chain does not have a fixed, deterministic cycle length.

Recurrence: A state in a Markov chain is recurrent if, once the chain leaves the state, it is guaranteed to return to the state infinitely often. If all states in a Markov chain are recurrent, then the chain is said to be positive recurrent.

When a Markov chain is irreducible, aperiodic, and positive recurrent, it has a unique limiting probability distribution. In such cases, the chain converges to the limiting probabilities as time goes to infinity, regardless of the initial state.

Limiting probabilities play an important role in the analysis of Markov chains and their applications in various fields, such as queueing theory, reliability analysis, and finance. They provide insights into the long-term behavior and steady-state properties of a stochastic system.
